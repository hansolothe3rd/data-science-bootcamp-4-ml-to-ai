# ğŸ§  Data Science Bootcamp â€“ Repo 4: From ML to AI

Welcome to the fourth stage of your 100-day data science journey!  
This repo transitions from traditional machine learning to **AI and deep learning concepts**, introducing you to neural networks, optimization, and real-world applications.

---

## ğŸ—“ï¸ Days Overview

| Day | Topic | Focus |
|-----|-------|--------|
| 31 | Gradient Descent & Optimization | How models learn through iterative improvement |
| 32 | Feature Engineering Deep Dive | Handling outliers, skewness, and transformations |
| 33 | Regularization (L1/L2/Ridge/Lasso) | Preventing overfitting |
| 34 | Decision Trees & Random Forests | Non-linear ML models |
| 35 | Gradient Boosting (XGBoost, LightGBM) | Advanced ensemble methods |
| 36 | Neural Network Basics | Intro to perceptrons and activation functions |
| 37 | Building an ANN with TensorFlow/Keras | First deep learning model |
| 38 | Improving Neural Nets | Dropout, batch norm, tuning |
| 39 | CNNs (Image Classification) | Convolutional Neural Networks intro |
| 40 | Mini Capstone Project | Build an end-to-end AI model project |

---

## Day 31 â€“ Gradient Descent & Optimization

**Focus:** Understanding how models learn by minimizing loss functions.  
**Topics Covered:**
- Concept and math of gradient descent  
- Learning rate and convergence  
- Manual implementation with Python  
- Mini project: Linear Regression from scratch  

**Skills Gained:**
- Mathematical intuition for optimization  
- Parameter tuning practice  
- Implementing ML fundamentals manually

---

## Day 32 â€“ Feature Engineering Deep Dive
**Focus:** Handling outliers, skewness, transformations, and scaling features.

**Mini Project:**  
- Dataset: California Housing  
- Steps:
  1. Detect and cap outliers (IQR method)  
  2. Fix skewed features (log transformation)  
  3. Scale numeric columns (StandardScaler)  
  4. Encode categorical features (one-hot)  
  5. Build preprocessing pipeline  
  6. Train Linear Regression and Random Forest models  
  7. Evaluate and visualize feature importances

**Key Takeaways:**  
- Clean, well-transformed features improve model performance  
- Scaling helps optimization  
- Pipelines make preprocessing reproducible

---

## ğŸ§  Day 33 â€” Data Preprocessing with scikit-learn

**Focus:** Cleaning and preparing data before modeling.  
**Key Tools:** `SimpleImputer`, `OneHotEncoder`, `StandardScaler`, `ColumnTransformer`, `Pipeline`

### ğŸ“š What I Learned
- Handle missing values using mean or most frequent imputation  
- Encode categorical variables with one-hot encoding  
- Standardize numeric features for consistent scaling  
- Combine steps into a preprocessing pipeline for cleaner workflows  

### ğŸ§© Mini Project
Preprocessed the **Titanic dataset** â€” imputing missing data, encoding categorical variables, scaling features, and splitting into train/test sets for modeling.

---

## ğŸŒ³ Day 34 â€” Decision Trees & Random Forests

**Focus:** Understanding tree-based models for non-linear decision making.  
**Key Tools:** `scikit-learn`, `pandas`, `matplotlib`, `seaborn`

### ğŸ“š What I Learned
- How **Decision Trees** split data using **Gini impurity** and **entropy**  
- Visualized trees and understood how splits affect bias and variance  
- Learned how **Random Forests** improve accuracy by averaging multiple trees  
- Compared model performance between Decision Trees and Random Forests  
- Tuned hyperparameters like `max_depth`, `n_estimators`, and `min_samples_split`

### ğŸ§© Mini Project
Built a **Customer Churn Predictor** using Random Forests:  
- Preprocessed categorical and numeric data  
- Trained and tuned models using cross-validation  
- Evaluated accuracy, precision, recall, and feature importance  

---

## ğŸš€ Day 35 â€” Gradient Boosting (XGBoost & LightGBM)

**Focus:** Mastering advanced ensemble methods for high-performance ML.  
**Key Tools:** `XGBoost`, `LightGBM`, `scikit-learn`, `matplotlib`

### ğŸ“š What I Learned
- How **Gradient Boosting** builds models sequentially to reduce errors  
- The difference between **Bagging (Random Forests)** and **Boosting (XGBoost)**  
- Tuned hyperparameters like:
  - `learning_rate`
  - `n_estimators`
  - `max_depth`
  - `subsample`
- Evaluated performance using metrics like accuracy, AUC, and confusion matrices  
- Compared **XGBoost** and **LightGBM** efficiency and accuracy  

### ğŸ§© Mini Project
Built a **Loan Default Prediction Model**:
- Cleaned and encoded financial dataset features  
- Trained models using XGBoost and LightGBM  
- Tuned hyperparameters with grid search  
- Visualized **feature importance** and **ROC curves**

---

## ğŸ§  Day 36 â€” Neural Network Basics

**Focus:** Understanding how neural networks learn through layers, weights, and activations.  
**Key Tools:** `NumPy`, `TensorFlow`, `Keras`, `matplotlib`

### ğŸ“š What I Learned
- The structure of a **Perceptron** â€” inputs, weights, bias, activation  
- Difference between **linear** and **non-linear** activation functions  
- How forward propagation computes predictions  
- How backpropagation adjusts weights through gradients  
- Used TensorFlow/Keras to build a simple feedforward neural network

### ğŸ§© Mini Project
Built a **Binary Classifier** to predict whether a student passes or fails based on study hours and sleep:
- Implemented a single-layer perceptron in NumPy  
- Recreated it using **Keras Sequential API**  
- Visualized the decision boundary and model performance


## ğŸ§© Skills Youâ€™ll Learn
- Model optimization and tuning  
- Deep learning with TensorFlow/Keras  
- Advanced feature engineering  
- Bias-variance tradeoff  
- Image and structured data modeling  
- Practical ML deployment techniques

---

# ğŸ§  Day 37 â€“ Building an ANN with TensorFlow/Keras

## ğŸ“Œ Overview
Today we expand on neural networks by building a **deeper artificial neural network (ANN)** with multiple hidden layers using TensorFlow/Keras.

---

## ğŸ”¹ Learning Objectives
- Understand the **Sequential model** in Keras  
- Add multiple **hidden layers**  
- Use different **activation functions**  
- Train and evaluate your ANN  
- Visualize training metrics like **loss** and **accuracy**

---

## ğŸ§© Key Concepts

### ANN Components
- **Input layer** â€“ receives features  
- **Hidden layers** â€“ learn patterns from data  
- **Output layer** â€“ predicts target values  

### Activation Functions
- **ReLU** â€“ common in hidden layers  
- **Sigmoid** â€“ good for binary outputs  
- **Softmax** â€“ for multi-class outputs  

### Architecture Example
- Input â†’ Hidden layer 1 (8 neurons, ReLU)  
- Hidden layer 2 (4 neurons, ReLU)  
- Output layer (1 neuron, Sigmoid)  

### Training Notes
- **Loss function:** binary_crossentropy  
- **Optimizer:** Adam  
- **Epochs:** 100+  
- **Batch size:** 32  

---

## ğŸ›  Notebook Tasks
1. Build a 2-hidden-layer ANN using TensorFlow/Keras  
2. Train the network for 100 epochs on a small dataset  
3. Track **loss** and **accuracy**  
4. Plot training curves  
5. Write a 2â€“4 sentence interpretation of results  

---

## ğŸ§ª Mini Project
- Build your own **2-hidden-layer ANN**  
- Experiment with **4â€“10 neurons per layer**  
- Try different **activation functions** (ReLU, Tanh, Sigmoid)  
- Train for 100+ epochs  
- Plot **loss and accuracy curves**  
- Write a brief **analysis of network performance**  

---

# ğŸ§  Day 38 â€“ Improving Neural Networks

## ğŸ“Œ Overview
Today we focus on improving your artificial neural networks (ANNs) using techniques like **dropout, batch normalization, and hyperparameter tuning**. These methods help prevent overfitting and make your model generalize better.

---

## ğŸ”¹ Learning Objectives
- Understand **overfitting** vs **underfitting**  
- Implement **dropout layers**  
- Apply **batch normalization**  
- Tune hyperparameters (neurons, activation functions, learning rate)  
- Evaluate model performance and improvements

---

## ğŸ§© Key Concepts

### Overfitting vs Underfitting
- **Overfitting:** model performs well on training data but poorly on new data  
- **Underfitting:** model fails to learn patterns, low performance overall  

### Dropout
- Randomly disables neurons during training  
- Prevents over-reliance on specific nodes  
- Typical dropout rate: 0.2â€“0.5  

### Batch Normalization
- Normalizes inputs to each layer  
- Speeds up training and stabilizes learning  

### Hyperparameter Tuning
- Adjust:  
  - Number of hidden layers and neurons  
  - Activation functions  
  - Learning rate  
  - Batch size and epochs  

---

## ğŸ›  Notebook Tasks
1. Add **dropout** to your existing ANN  
2. Apply **batch normalization** to hidden layers  
3. Tune hyperparameters and re-train  
4. Compare **loss and accuracy curves** with original model  
5. Write a short interpretation (2â€“4 sentences)  

---

## ğŸ§ª Mini Project
- Take your **Day 37 ANN**  
- Add **dropout** (0.2â€“0.5) to hidden layers  
- Add **batch normalization**  
- Train for 100+ epochs  
- Plot **loss and accuracy curves**  
- Experiment with:  
  - Different numbers of neurons per layer (4â€“10)  
  - Different activation functions (ReLU, Tanh, Sigmoid)  
  - Different learning rates (0.001â€“0.01)  
- Write a brief analysis of performance improvements

---

## âš™ï¸ Setup
- Install TensorFlow:  
```bash
pip install tensorflow


## âš™ï¸ Setup
- Install TensorFlow:  
```bash
pip install tensorflow


## âš™ï¸ Setup
```bash
git clone https://github.com/yourusername/data-science-bootcamp-4-ml-to-ai.git
cd data-science-bootcamp-4-ml-to-ai
